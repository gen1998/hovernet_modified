{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import copy\n",
    "from misc.utils import log_info\n",
    "from docopt import docopt\n",
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "from multiprocessing import Lock, Pool\n",
    "\n",
    "multiprocessing.set_start_method(\"spawn\", True)  # ! must be at top for VScode debugging\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from concurrent.futures import FIRST_EXCEPTION, ProcessPoolExecutor, as_completed, wait\n",
    "from functools import reduce\n",
    "from importlib import import_module\n",
    "from multiprocessing import Lock, Pool\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import psutil\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import tqdm\n",
    "from dataloader.infer_loader import SerializeArray, SerializeFileList\n",
    "from misc.utils import (\n",
    "    color_deconvolution,\n",
    "    cropping_center,\n",
    "    get_bounding_box,\n",
    "    log_debug,\n",
    "    log_info,\n",
    "    rm_n_mkdir,\n",
    ")\n",
    "from misc.viz_utils import colorize, visualize_instances_dict\n",
    "from skimage import color\n",
    "\n",
    "import convert_format\n",
    "from infer import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "nr_gpus = torch.cuda.device_count()\n",
    "\n",
    "batch_size = 32\n",
    "nr_types = 6\n",
    "mem_usage = 0.1\n",
    "\n",
    "model_mode = \"fast\"\n",
    "sub_cmd = \"tile\"\n",
    "\n",
    "origin_path = \"/home/takagi/program/hover_net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_args = {\n",
    "    'method' : {\n",
    "        'model_args' : {\n",
    "            'nr_types'   : int(nr_types),\n",
    "            'mode'       : model_mode,\n",
    "        },\n",
    "        'model_path' : os.path.join(origin_path, \"weights/hovernet_fast_pannuke_type_tf2pytorch.tar\"),\n",
    "    },\n",
    "    'type_info_path'  : \"type_info.json\",\n",
    "}\n",
    "\n",
    "run_args = {\n",
    "    'batch_size' : int(batch_size) * nr_gpus,\n",
    "\n",
    "    'nr_inference_workers' : int(8),\n",
    "    'nr_post_proc_workers' : int(16),\n",
    "}\n",
    "\n",
    "if model_mode == 'fast':\n",
    "    run_args['patch_input_shape'] = 256\n",
    "    run_args['patch_output_shape'] = 164\n",
    "else:\n",
    "    run_args['patch_input_shape'] = 270\n",
    "    run_args['patch_output_shape'] = 80\n",
    "\n",
    "if sub_cmd == 'tile':\n",
    "    run_args.update({\n",
    "        'input_dir'      : \"/home/takagi/data/iga/roi/img\",\n",
    "        'output_dir'     : \"/home/takagi/data/iga/hover_net/roi_original/pannuke\",\n",
    "\n",
    "        'mem_usage'   : float(mem_usage),\n",
    "        'draw_dot'    : \"\",\n",
    "        'save_qupath' : \"\",\n",
    "        'save_raw_map': \"\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_patching(img, window_size, mask_size, return_src_top_corner=False):\n",
    "    \"\"\"Prepare patch information for tile processing.\n",
    "    \n",
    "    Args:\n",
    "        img: original input image\n",
    "        window_size: input patch size\n",
    "        mask_size: output patch size\n",
    "        return_src_top_corner: whether to return coordiante information for top left corner of img\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    win_size = window_size\n",
    "    msk_size = step_size = mask_size\n",
    "\n",
    "    def get_last_steps(length, msk_size, step_size):\n",
    "        nr_step = math.ceil((length - msk_size) / step_size)\n",
    "        last_step = (nr_step + 1) * step_size\n",
    "        return int(last_step), int(nr_step + 1)\n",
    "\n",
    "    im_h = img.shape[0]\n",
    "    im_w = img.shape[1]\n",
    "\n",
    "    last_h, _ = get_last_steps(im_h, msk_size, step_size)\n",
    "    last_w, _ = get_last_steps(im_w, msk_size, step_size)\n",
    "\n",
    "    diff = win_size - step_size\n",
    "    padt = padl = diff // 2\n",
    "    padb = last_h + win_size - im_h\n",
    "    padr = last_w + win_size - im_w\n",
    "\n",
    "    img = np.lib.pad(img, ((padt, padb), (padl, padr), (0, 0)), \"reflect\")\n",
    "\n",
    "    # generating subpatches index from orginal\n",
    "    coord_y = np.arange(0, last_h, step_size, dtype=np.int32)\n",
    "    coord_x = np.arange(0, last_w, step_size, dtype=np.int32)\n",
    "    row_idx = np.arange(0, coord_y.shape[0], dtype=np.int32)\n",
    "    col_idx = np.arange(0, coord_x.shape[0], dtype=np.int32)\n",
    "    coord_y, coord_x = np.meshgrid(coord_y, coord_x)\n",
    "    row_idx, col_idx = np.meshgrid(row_idx, col_idx)\n",
    "    coord_y = coord_y.flatten()\n",
    "    coord_x = coord_x.flatten()\n",
    "    row_idx = row_idx.flatten()\n",
    "    col_idx = col_idx.flatten()\n",
    "    #\n",
    "    patch_info = np.stack([coord_y, coord_x, row_idx, col_idx], axis=-1)\n",
    "    if not return_src_top_corner:\n",
    "        return img, patch_info\n",
    "    else:\n",
    "        return img, patch_info, [padt, padl]\n",
    "\n",
    "\n",
    "####\n",
    "def _post_process_patches(\n",
    "    post_proc_func, post_proc_kwargs, patch_info, image_info, overlay_kwargs,\n",
    "):\n",
    "    \"\"\"Apply post processing to patches.\n",
    "    \n",
    "    Args:\n",
    "        post_proc_func: post processing function to use\n",
    "        post_proc_kwargs: keyword arguments used in post processing function\n",
    "        patch_info: patch data and associated information\n",
    "        image_info: input image data and associated information\n",
    "        overlay_kwargs: overlay keyword arguments\n",
    "\n",
    "    \"\"\"\n",
    "    # re-assemble the prediction, sort according to the patch location within the original image\n",
    "    patch_info = sorted(patch_info, key=lambda x: [x[0][0], x[0][1]])\n",
    "    patch_info, patch_data = zip(*patch_info)\n",
    "\n",
    "    src_shape = image_info[\"src_shape\"]\n",
    "    src_image = image_info[\"src_image\"]\n",
    "\n",
    "    patch_shape = np.squeeze(patch_data[0]).shape\n",
    "    ch = 1 if len(patch_shape) == 2 else patch_shape[-1]\n",
    "    axes = [0, 2, 1, 3, 4] if ch != 1 else [0, 2, 1, 3]\n",
    "\n",
    "    nr_row = max([x[2] for x in patch_info]) + 1\n",
    "    nr_col = max([x[3] for x in patch_info]) + 1\n",
    "    pred_map = np.concatenate(patch_data, axis=0)\n",
    "    pred_map = np.reshape(pred_map, (nr_row, nr_col) + patch_shape)\n",
    "    pred_map = np.transpose(pred_map, axes)\n",
    "    pred_map = np.reshape(\n",
    "        pred_map, (patch_shape[0] * nr_row, patch_shape[1] * nr_col, ch)\n",
    "    )\n",
    "    # crop back to original shape\n",
    "    pred_map = np.squeeze(pred_map[: src_shape[0], : src_shape[1]])\n",
    "\n",
    "    # * Implicit protocol\n",
    "    # * a prediction map with instance of ID 1-N\n",
    "    # * and a dict contain the instance info, access via its ID\n",
    "    # * each instance may have type\n",
    "    pred_inst, inst_info_dict = post_proc_func(pred_map, **post_proc_kwargs)\n",
    "\n",
    "    overlaid_img = visualize_instances_dict(\n",
    "        src_image.copy(), inst_info_dict, **overlay_kwargs\n",
    "    )\n",
    "\n",
    "    return image_info[\"name\"], pred_map, pred_inst, inst_info_dict, overlaid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferManager(base.InferManager):\n",
    "    \"\"\"Run inference on tiles.\"\"\"\n",
    "\n",
    "    ####\n",
    "    def process_file_list(self, run_args):\n",
    "        \"\"\"\n",
    "        Process a single image tile < 5000x5000 in size.\n",
    "        \"\"\"\n",
    "        for variable, value in run_args.items():\n",
    "            self.__setattr__(variable, value)\n",
    "        assert self.mem_usage < 1.0 and self.mem_usage > 0.0\n",
    "\n",
    "        # * depend on the number of samples and their size, this may be less efficient\n",
    "        patterning = lambda x: re.sub(\"([\\[\\]])\", \"[\\\\1]\", x)\n",
    "        file_path_list = glob.glob(patterning(\"%s/*\" % self.input_dir))\n",
    "        file_path_list.sort()  # ensure same order\n",
    "        assert len(file_path_list) > 0, 'Not Detected Any Files From Path'\n",
    "        \n",
    "        rm_n_mkdir(self.output_dir + '/json/')\n",
    "        rm_n_mkdir(self.output_dir + '/mat/')\n",
    "        rm_n_mkdir(self.output_dir + '/overlay/')\n",
    "        if self.save_qupath:\n",
    "            rm_n_mkdir(self.output_dir + \"/qupath/\")\n",
    "\n",
    "        def proc_callback(results):\n",
    "            \"\"\"Post processing callback.\n",
    "            \n",
    "            Output format is implicit assumption, taken from `_post_process_patches`\n",
    "\n",
    "            \"\"\"\n",
    "            img_name, pred_map, pred_inst, inst_info_dict, overlaid_img = results\n",
    "\n",
    "            nuc_val_list = list(inst_info_dict.values())\n",
    "            # need singleton to make matlab happy\n",
    "            nuc_uid_list = np.array(list(inst_info_dict.keys()))[:,None]\n",
    "            nuc_type_list = np.array([v[\"type\"] for v in nuc_val_list])[:,None]\n",
    "            nuc_coms_list = np.array([v[\"centroid\"] for v in nuc_val_list])\n",
    "\n",
    "            mat_dict = {\n",
    "                \"inst_map\" : pred_inst,\n",
    "                \"inst_uid\" : nuc_uid_list,\n",
    "                \"inst_type\": nuc_type_list,\n",
    "                \"inst_centroid\": nuc_coms_list\n",
    "            }\n",
    "            if self.nr_types is None: # matlab does not have None type array\n",
    "                mat_dict.pop(\"inst_type\", None) \n",
    "\n",
    "            if self.save_raw_map:\n",
    "                mat_dict[\"raw_map\"] = pred_map\n",
    "            save_path = \"%s/mat/%s.mat\" % (self.output_dir, img_name)\n",
    "            sio.savemat(save_path, mat_dict)\n",
    "\n",
    "            save_path = \"%s/overlay/%s.png\" % (self.output_dir, img_name)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(overlaid_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            if self.save_qupath:\n",
    "                nuc_val_list = list(inst_info_dict.values())\n",
    "                nuc_type_list = np.array([v[\"type\"] for v in nuc_val_list])\n",
    "                nuc_coms_list = np.array([v[\"centroid\"] for v in nuc_val_list])\n",
    "                save_path = \"%s/qupath/%s.tsv\" % (self.output_dir, img_name)\n",
    "                convert_format.to_qupath(\n",
    "                    save_path, nuc_coms_list, nuc_type_list, self.type_info_dict\n",
    "                )\n",
    "\n",
    "            save_path = \"%s/json/%s.json\" % (self.output_dir, img_name)\n",
    "            self.__save_json(save_path, inst_info_dict, None)\n",
    "            return img_name\n",
    "\n",
    "        def detach_items_of_uid(items_list, uid, nr_expected_items):\n",
    "            item_counter = 0\n",
    "            detached_items_list = []\n",
    "            remained_items_list = []\n",
    "            while True:\n",
    "                pinfo, pdata = items_list.pop(0)\n",
    "                pinfo = np.squeeze(pinfo)\n",
    "                if pinfo[-1] == uid:\n",
    "                    detached_items_list.append([pinfo, pdata])\n",
    "                    item_counter += 1\n",
    "                else:\n",
    "                    remained_items_list.append([pinfo, pdata])\n",
    "                if item_counter == nr_expected_items:\n",
    "                    break\n",
    "            # do this to ensure the ordering\n",
    "            remained_items_list = remained_items_list + items_list\n",
    "            return detached_items_list, remained_items_list\n",
    "\n",
    "        proc_pool = None\n",
    "        if self.nr_post_proc_workers > 0:\n",
    "            proc_pool = ProcessPoolExecutor(self.nr_post_proc_workers)\n",
    "\n",
    "        print(\"file_path_list\", len(file_path_list))\n",
    "\n",
    "        while len(file_path_list) > 0:\n",
    "\n",
    "            hardware_stats = psutil.virtual_memory()\n",
    "            available_ram = getattr(hardware_stats, \"available\")\n",
    "            print(\"available_ram\", available_ram)\n",
    "            available_ram = int(available_ram * self.mem_usage)\n",
    "            print(\"available_ram\", available_ram)\n",
    "            file_idx = 0\n",
    "            use_path_list = []\n",
    "            cache_image_list = []\n",
    "            cache_patch_info_list = []\n",
    "            cache_image_info_list = []\n",
    "            while len(file_path_list) > 0:\n",
    "                file_path = file_path_list.pop(0)\n",
    "\n",
    "                img = cv2.imread(file_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                src_shape = img.shape\n",
    "\n",
    "                img, patch_info, top_corner = _prepare_patching(\n",
    "                    img, self.patch_input_shape, self.patch_output_shape, True\n",
    "                )\n",
    "                self_idx = np.full(patch_info.shape[0], file_idx, dtype=np.int32)\n",
    "                patch_info = np.concatenate([patch_info, self_idx[:, None]], axis=-1)\n",
    "                # ? may be expensive op\n",
    "                patch_info = np.split(patch_info, patch_info.shape[0], axis=0)\n",
    "                patch_info = [np.squeeze(p) for p in patch_info]\n",
    "\n",
    "                # * this factor=5 is only applicable for HoVerNet\n",
    "                expected_usage = sys.getsizeof(img) * 5\n",
    "                available_ram -= expected_usage\n",
    "                if available_ram < 0:\n",
    "                    file_path_list.insert(0, file_path)\n",
    "                    break\n",
    "\n",
    "                file_idx += 1\n",
    "                # if file_idx == 4: break\n",
    "                use_path_list.append(file_path)\n",
    "                cache_image_list.append(img)\n",
    "                cache_patch_info_list.extend(patch_info)\n",
    "                # TODO: refactor to explicit protocol\n",
    "                cache_image_info_list.append([src_shape, len(patch_info), top_corner])\n",
    "\n",
    "            print(\"cache\", len(cache_image_info_list), len(file_path_list), len(use_path_list))\n",
    "            print(\"available_ram\", available_ram)\n",
    "            # * apply neural net on cached data\n",
    "            dataset = SerializeFileList(\n",
    "                cache_image_list, cache_patch_info_list, self.patch_input_shape\n",
    "            )\n",
    "\n",
    "            dataloader = data.DataLoader(\n",
    "                dataset,\n",
    "                num_workers=self.nr_inference_workers,\n",
    "                batch_size=self.batch_size,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            pbar = tqdm.tqdm(\n",
    "                desc=\"Process Patches\",\n",
    "                leave=True,\n",
    "                total=int(len(cache_patch_info_list) / self.batch_size) + 1,\n",
    "                ncols=80,\n",
    "                ascii=True,\n",
    "                position=0,\n",
    "            )\n",
    "\n",
    "            accumulated_patch_output = []\n",
    "            for batch_idx, batch_data in enumerate(dataloader):\n",
    "                sample_data_list, sample_info_list = batch_data\n",
    "                sample_output_list = self.run_step(sample_data_list)\n",
    "                sample_info_list = sample_info_list.numpy()\n",
    "                curr_batch_size = sample_output_list.shape[0]\n",
    "                sample_output_list = np.split(\n",
    "                    sample_output_list, curr_batch_size, axis=0\n",
    "                )\n",
    "                sample_info_list = np.split(sample_info_list, curr_batch_size, axis=0)\n",
    "                sample_output_list = list(zip(sample_info_list, sample_output_list))\n",
    "                accumulated_patch_output.extend(sample_output_list)\n",
    "                pbar.update()\n",
    "            pbar.close()\n",
    "\n",
    "            # * parallely assemble the processed cache data for each file if possible\n",
    "            future_list = []\n",
    "            for file_idx, file_path in enumerate(use_path_list):\n",
    "                image_info = cache_image_info_list[file_idx]\n",
    "                file_ouput_data, accumulated_patch_output = detach_items_of_uid(\n",
    "                    accumulated_patch_output, file_idx, image_info[1]\n",
    "                )\n",
    "\n",
    "                # * detach this into func and multiproc dispatch it\n",
    "                src_pos = image_info[2]  # src top left corner within padded image\n",
    "                src_image = cache_image_list[file_idx]\n",
    "                src_image = src_image[\n",
    "                    src_pos[0] : src_pos[0] + image_info[0][0],\n",
    "                    src_pos[1] : src_pos[1] + image_info[0][1],\n",
    "                ]\n",
    "\n",
    "                base_name = pathlib.Path(file_path).stem\n",
    "                file_info = {\n",
    "                    \"src_shape\": image_info[0],\n",
    "                    \"src_image\": src_image,\n",
    "                    \"name\": base_name,\n",
    "                }\n",
    "\n",
    "                post_proc_kwargs = {\n",
    "                    \"nr_types\": self.nr_types,\n",
    "                    \"return_centroids\": True,\n",
    "                }  # dynamicalize this\n",
    "\n",
    "                overlay_kwargs = {\n",
    "                    \"draw_dot\": self.draw_dot,\n",
    "                    \"type_colour\": self.type_info_dict,\n",
    "                    \"line_thickness\": 2,\n",
    "                }\n",
    "                func_args = (\n",
    "                    self.post_proc_func,\n",
    "                    post_proc_kwargs,\n",
    "                    file_ouput_data,\n",
    "                    file_info,\n",
    "                    overlay_kwargs,\n",
    "                )\n",
    "\n",
    "                # dispatch for parallel post-processing\n",
    "                if proc_pool is not None:\n",
    "                    proc_future = proc_pool.submit(_post_process_patches, *func_args)\n",
    "                    # ! manually poll future and call callback later as there is no guarantee\n",
    "                    # ! that the callback is called from main thread\n",
    "                    future_list.append(proc_future)\n",
    "                else:\n",
    "                    proc_output = _post_process_patches(*func_args)\n",
    "                    proc_callback(proc_output)\n",
    "\n",
    "            if proc_pool is not None:\n",
    "                # loop over all to check state a.k.a polling\n",
    "                for future in as_completed(future_list):\n",
    "                    # TODO: way to retrieve which file crashed ?\n",
    "                    # ! silent crash, cancel all and raise error\n",
    "                    if future.exception() is not None:\n",
    "                        log_info(\"Silent Crash\")\n",
    "                        # ! cancel somehow leads to cascade error later\n",
    "                        # ! so just poll it then crash once all future\n",
    "                        # ! acquired for now\n",
    "                        # for future in future_list:\n",
    "                        #     future.cancel()\n",
    "                        # break\n",
    "                    else:\n",
    "                        file_path = proc_callback(future.result())\n",
    "                        log_info(\"Done Assembling %s\" % file_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path_list 10293\n",
      "available_ram 16931643392\n",
      "available_ram 1693164339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Patches:   0%|                                   | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache 34 10259 34\n",
      "available_ram -34895561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Patches: 96it [01:47,  1.12s/it]                                        \n",
      "Process SpawnProcess-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/takagi/anaconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/takagi/anaconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/takagi/anaconda3/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/takagi/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute '_post_process_patches' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_ram 15467573248\n",
      "available_ram 1546757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Patches:   0%|                                   | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache 31 10228 31\n",
      "available_ram -33183156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Patches: 88it [01:38,  1.12s/it]                                        \n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m infer \u001b[38;5;241m=\u001b[39m InferManager(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmethod_args)\n\u001b[0;32m----> 2\u001b[0m \u001b[43minfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 217\u001b[0m, in \u001b[0;36mInferManager.process_file_list\u001b[0;34m(self, run_args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# dispatch for parallel post-processing\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc_pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     proc_future \u001b[38;5;241m=\u001b[39m \u001b[43mproc_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_post_process_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# ! manually poll future and call callback later as there is no guarantee\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ! that the callback is called from main thread\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     future_list\u001b[38;5;241m.\u001b[39mappend(proc_future)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/concurrent/futures/process.py:629\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_lock:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[0;32m--> 629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot schedule new futures after shutdown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "infer = InferManager(**method_args)\n",
    "infer.process_file_list(run_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
